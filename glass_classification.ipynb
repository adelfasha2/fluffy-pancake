{"cells":[{"metadata":{"_cell_guid":"6d3603b1-2e19-47f3-90e7-fa4b017020a9","trusted":false},"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  # read and wrangle dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.base import TransformerMixin # To create new classes for transformations\nfrom sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.stats import boxcox # data transform\nfrom sklearn.model_selection import (train_test_split, KFold , StratifiedKFold, \n                                     cross_val_score, GridSearchCV, \n                                     learning_curve, validation_curve) # model selection modules\nfrom sklearn.pipeline import Pipeline # streaming pipelines\nfrom sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\nfrom collections import Counter\nimport warnings\n# load models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import (XGBClassifier, plot_importance)\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom time import time\n\n%matplotlib inline \nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd4f341c-f072-efea-f0f5-891184db0a07","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/glass.csv')\nfeatures = df.columns[:-1].tolist()\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0aa76eab-6060-b7f4-338e-23a5c543a68c","trusted":false},"cell_type":"code","source":"df.head(15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c4b9854-55f6-2dc6-fd4c-cd0aea51f583","trusted":false},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4c5e49d9-fb5a-3147-14c8-546c3b0b852c","trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43f288e9-3640-7e9a-6707-cab227c611f3","trusted":false},"cell_type":"code","source":"df['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d530a3ab-0a42-f70f-83d3-ee102bc4a7a1","trusted":false},"cell_type":"code","source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde= False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93df2e16-2776-5dd8-4b02-6e3caa8472ff","trusted":false},"cell_type":"code","source":"# Detect observations with more than one outlier\n\ndef outlier_hunt(df):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than 2 outliers. \n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in df.columns.tolist():\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        \n        # Interquartile rrange (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n    \n    return multiple_outliers   \n\nprint('The dataset contains %d observations with more than 2 outliers' %(len(outlier_hunt(df[features]))))   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41e7eab7-417b-65cc-f8f9-5430bffcfc29","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.boxplot(df[features])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d51b128-d4f4-805e-ff4f-721f0a0659b8","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.pairplot(df[features],palette='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4da104e6-ebeb-9e60-be90-1d989eddb19a","trusted":false},"cell_type":"code","source":"corr = df[features].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0b2b3999-b0b2-c2ec-4481-559822cfa9ee","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"faf410bf-5705-d9f5-1526-1d72f47c58a2","trusted":false},"cell_type":"code","source":"outlier_indices = outlier_hunt(df[features])\ndf = df.drop(outlier_indices).reset_index(drop=True)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f62554c7-f123-fed6-7917-9d99fbe988a0","trusted":false},"cell_type":"code","source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde=False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e98d7b4-b9fe-cd2b-762d-65ed27585230","trusted":false},"cell_type":"code","source":"df['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b487fa72-e6d1-2a35-e5f6-c14fed1d0bcd","trusted":false},"cell_type":"code","source":"sns.countplot(df['Type'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a19d0852-9efa-061e-b7df-8a99342b63a0","trusted":false},"cell_type":"code","source":"# Define X as features and y as lablels\nX = df[features] \ny = df['Type'] \n# set a seed and a test size for splitting the dataset \nseed = 7\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6584fd74-5cf7-5f5e-c33f-8cc475c05dfa","trusted":false},"cell_type":"code","source":"features_boxcox = []\n\nfor feature in features:\n    bc_transformed, _ = boxcox(df[feature]+1)  # shift by 1 to avoid computing log of negative values\n    features_boxcox.append(bc_transformed)\n\nfeatures_boxcox = np.column_stack(features_boxcox)\ndf_bc = pd.DataFrame(data=features_boxcox, columns=features)\ndf_bc['Type'] = df['Type']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89c960c7-d9bd-1bce-a245-70d99daeacb0","trusted":false},"cell_type":"code","source":"df_bc.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1694825f-1840-bd01-7f25-5eb255120f74","trusted":false},"cell_type":"code","source":"for feature in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(df[feature], color='blue', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df[feature].skew(),3))) )\n    ax[0].set_title(str(feature))   \n    ax[0].legend(loc=0)\n    ax[1].hist(df_bc[feature], color='red', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df_bc[feature].skew(),3))) )\n    ax[1].set_title(str(feature)+' after a Box-Cox transformation')\n    ax[1].legend(loc=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30706cae-af03-6481-9a32-50b3d313f85e","trusted":false},"cell_type":"code","source":"# check if skew is closer to zero after a box-cox transform\nfor feature in features:\n    delta = np.abs( df_bc[feature].skew() / df[feature].skew() )\n    if delta < 1.0 :\n        print('Feature %s is less skewed after a Box-Cox transform' %(feature))\n    else:\n        print('Feature %s is more skewed after a Box-Cox transform'  %(feature))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddc8ecf6-6ed6-56e8-0cb2-09fa714c95ea","trusted":false},"cell_type":"code","source":"model_importances = XGBClassifier()\nstart = time()\nmodel_importances.fit(X_train, y_train)\nprint('Elapsed time to train XGBoost  %.3f seconds' %(time()-start))\nplot_importance(model_importances)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"314cbd53-dc8c-8178-26d9-231ccacbd414","trusted":false},"cell_type":"code","source":"pca = PCA(random_state = seed)\npca.fit(X_train)\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\nplt.figure(figsize=(8,6))\nplt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n       alpha = 0.7)\nplt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n        color= 'red')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xticks(np.arange(1,len(var_exp)+1,1))\nplt.legend(loc='center right')\nplt.show()\n\n# Cumulative variance explained\nfor i, sum in enumerate(cum_var_exp):\n    print(\"PC\" + str(i+1), \"Cumulative variance: %.3f% %\" %(cum_var_exp[i]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae576fd3-697e-88a2-294b-d08a60adcc84","trusted":false},"cell_type":"code","source":"n_components = 5\npipelines = []\nn_estimators = 200\n\n#print(df.shape)\npipelines.append( ('SVC',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                             ('SVC', SVC(random_state=seed))]) ) )\n\n\npipelines.append(('KNN',\n                  Pipeline([ \n                              ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                            ('KNN', KNeighborsClassifier()) ])))\npipelines.append( ('RF',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('RF', RandomForestClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n\n\npipelines.append( ('Ada',\n                   Pipeline([ \n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                    ('Ada', AdaBoostClassifier(random_state=seed,  n_estimators=n_estimators)) ]) ))\n\npipelines.append( ('ET',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('ET', ExtraTreesClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\npipelines.append( ('GB',\n                   Pipeline([ \n                             ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n\npipelines.append( ('LR',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n\nresults, names, times  = [], [] , []\nnum_folds = 10\nscoring = 'accuracy'\n\nfor name, model in pipelines:\n    start = time()\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n                                n_jobs=-1) \n    t_elapsed = time() - start\n    results.append(cv_results)\n    names.append(name)\n    times.append(t_elapsed)\n    msg = \"%s: %f (+/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n                                                       100*cv_results.std(), t_elapsed)\n    print(msg)\n\n\nfig = plt.figure(figsize=(12,8))    \nfig.suptitle(\"Algorithms comparison\")\nax = fig.add_subplot(1,1,1)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa261f12-a4a0-df09-1bd2-c3a2151ea6e7","trusted":false},"cell_type":"code","source":"# Create a pipeline with a Random forest classifier\npipe_rfc = Pipeline([ \n                      ('scl', StandardScaler()), \n                    ('rfc', RandomForestClassifier(random_state=seed, n_jobs=-1) )])\n\n# Set the grid parameters\nparam_grid_rfc =  [ {\n    'rfc__n_estimators': [100, 200,300,400], # number of estimators\n    #'rfc__criterion': ['gini', 'entropy'],   # Splitting criterion\n    'rfc__max_features':[0.05 , 0.1], # maximum features used at each split\n    'rfc__max_depth': [None, 5], # Max depth of the trees\n    'rfc__min_samples_split': [0.005, 0.01], # mininal samples in leafs\n    }]\n# Use 10 fold CV\nkfold = StratifiedKFold(n_splits=num_folds, random_state= seed)\ngrid_rfc = GridSearchCV(pipe_rfc, param_grid= param_grid_rfc, cv=kfold, scoring=scoring, verbose= 1, n_jobs=-1)\n\n#Fit the pipeline\nstart = time()\ngrid_rfc = grid_rfc.fit(X_train, y_train)\nend = time()\n\nprint(\"RFC grid search took %.3f seconds\" %(end-start))\n\n# Best score and best parameters\nprint('-------Best score----------')\nprint(grid_rfc.best_score_ * 100.0)\nprint('-------Best params----------')\nprint(grid_rfc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5eb36f01-70b6-be52-362f-1d2d635ec45e","trusted":false},"cell_type":"code","source":"# Let's define some utility functions to plot the learning & validation curves\n\ndef plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(train_sizes,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(train_sizes, test_mean, label='test score', color='red',marker='o')\n    plt.fill_between(train_sizes,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.xlabel('Number of training points')\n    plt.ylabel('Accuracy')\n    plt.grid(ls='--')\n    plt.legend(loc='best')\n    plt.show()    \n    \ndef plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(param_range,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(param_range, test_mean, label='test score', color='red', marker='o')\n    plt.fill_between(param_range,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.grid(ls='--')\n    plt.xlabel('Parameter value')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f03b5dc9-99c1-4361-bf7a-0f516d3270f5","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(9,6))\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n              estimator= grid_rfc.best_estimator_ , X= X_train, y = y_train, \n                train_sizes=np.arange(0.1,1.1,0.1), cv= 10,  scoring='accuracy', n_jobs= - 1)\n\nplot_learning_curve(train_sizes, train_scores, test_scores, title='Learning curve for RFC')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5771670f-c60e-f98d-d667-c28b8b3baa1d","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}